{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the Quickstart tutorial from PyTorch ([link](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)) for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from sniper import SniperTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim_lr = 1e-3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SNIPER example starts here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Define sparsity schedule\n",
    "schedule = {0: 40,\n",
    "            5: 20,\n",
    "            10: 10,\n",
    "            20: 0}\n",
    "\n",
    "# Define variables needed for SNIPER training\n",
    "model_builder = NeuralNetwork\n",
    "snip_module_name = ''\n",
    "batch_iterator = train_dataloader\n",
    "def get_loss_fn(model, batch):\n",
    "    X, y = batch\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    pred = model(X)\n",
    "    return loss_fn(pred, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Set logger\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The below will save model initial values, and the masks of the scheduled sparsities, in `masks`.\n",
    "\n",
    "This is a minimal example; there are more options such as setting maximum sparsity of parameters (see documentation on `sniper.train()` for more info)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial model state from masks/init_values.pt\n",
      "Loading initial model state from masks/init_values.pt\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "All required sparsities present, loading sparsity 40...\n",
      "All required sparsities present, loading sparsity 40...\n",
      "Loaded mask from masks/masks_40.pt\n",
      "Loaded mask from masks/masks_40.pt\n",
      "Adding mask operation to forward hook...\n",
      "Adding mask operation to forward hook...\n",
      "Creating optimizer learning rates\n",
      "Creating optimizer learning rates\n"
     ]
    }
   ],
   "source": [
    "sniper_dir = Path('masks')\n",
    "sniper = SniperTraining(sniper_dir=sniper_dir, logger=logger)\n",
    "sniper.train(\n",
    "    schedule=schedule,\n",
    "    model=model,\n",
    "    model_builder=model_builder,\n",
    "    snip_module_name=snip_module_name,\n",
    "    batch_iterator=batch_iterator,\n",
    "    get_loss_fn=get_loss_fn,\n",
    "    optim_lr=optim_lr,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimizer needs to accept the param groups from `sniper` to modify learning rates. If you do not have a scheduler, `sniper` will **directly** modify the learning rates during sparsity changes. If you have a scheduler, you need to set it in `sniper` to change  `base_lrs` when sparsity changes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0016033872578390255\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    name: linear_relu_stack.0.weight\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0011403118040089087\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    name: linear_relu_stack.0.bias\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001791996500006836\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    name: linear_relu_stack.2.weight\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 3\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0011797235023041474\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    name: linear_relu_stack.2.bias\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 4\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0011918063314711358\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    name: linear_relu_stack.4.weight\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 5\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0011111111111111111\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    name: linear_relu_stack.4.bias\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = model.parameters() if sniper.param_groups is None else sniper.param_groups\n",
    "optimizer = torch.optim.SGD(params, lr=optim_lr)\n",
    "print(optimizer)\n",
    "sniper.optimizers = [optimizer]\n",
    "# sniper.schedulers = [scheduler]  # if you are using a scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add `sniper.step()` to your training loop."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305630  [    0/60000]\n",
      "loss: 2.286907  [ 6400/60000]\n",
      "loss: 2.260293  [12800/60000]\n",
      "loss: 2.249516  [19200/60000]\n",
      "loss: 2.217139  [25600/60000]\n",
      "loss: 2.189889  [32000/60000]\n",
      "loss: 2.196691  [38400/60000]\n",
      "loss: 2.146585  [44800/60000]\n",
      "loss: 2.130537  [51200/60000]\n",
      "loss: 2.072554  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 2.069967 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.093181  [    0/60000]\n",
      "loss: 2.062598  [ 6400/60000]\n",
      "loss: 1.970859  [12800/60000]\n",
      "loss: 1.975963  [19200/60000]\n",
      "loss: 1.869963  [25600/60000]\n",
      "loss: 1.817361  [32000/60000]\n",
      "loss: 1.816961  [38400/60000]\n",
      "loss: 1.706102  [44800/60000]\n",
      "loss: 1.695969  [51200/60000]\n",
      "loss: 1.580570  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.600097 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.664915  [    0/60000]\n",
      "loss: 1.613560  [ 6400/60000]\n",
      "loss: 1.459231  [12800/60000]\n",
      "loss: 1.506228  [19200/60000]\n",
      "loss: 1.377245  [25600/60000]\n",
      "loss: 1.374366  [32000/60000]\n",
      "loss: 1.369921  [38400/60000]\n",
      "loss: 1.287176  [44800/60000]\n",
      "loss: 1.311564  [51200/60000]\n",
      "loss: 1.202111  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.236636 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.319652  [    0/60000]\n",
      "loss: 1.294163  [ 6400/60000]\n",
      "loss: 1.121510  [12800/60000]\n",
      "loss: 1.219453  [19200/60000]\n",
      "loss: 1.090237  [25600/60000]\n",
      "loss: 1.122688  [32000/60000]\n",
      "loss: 1.135287  [38400/60000]\n",
      "loss: 1.067608  [44800/60000]\n",
      "loss: 1.107459  [51200/60000]\n",
      "loss: 1.017982  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.043968 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.117884  [    0/60000]\n",
      "loss: 1.122178  [ 6400/60000]\n",
      "loss: 0.926679  [12800/60000]\n",
      "loss: 1.064251  [19200/60000]\n",
      "loss: 0.940469  [25600/60000]\n",
      "loss: 0.971935  [32000/60000]\n",
      "loss: 1.010129  [38400/60000]\n",
      "loss: 0.946025  [44800/60000]\n",
      "loss: 0.985727  [51200/60000]\n",
      "loss: 0.915161  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.931789 \n",
      "\n",
      "New sparsity scheduled: 20 -- replacing with new mask\n",
      "New sparsity scheduled: 20 -- replacing with new mask\n",
      "Loaded mask from masks/masks_20.pt\n",
      "Loaded mask from masks/masks_20.pt\n",
      "Setting new learning rates\n",
      "Setting new learning rates\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.997845  [    0/60000]\n",
      "loss: 1.030443  [ 6400/60000]\n",
      "loss: 0.815725  [12800/60000]\n",
      "loss: 0.975989  [19200/60000]\n",
      "loss: 0.864528  [25600/60000]\n",
      "loss: 0.882728  [32000/60000]\n",
      "loss: 0.943861  [38400/60000]\n",
      "loss: 0.882273  [44800/60000]\n",
      "loss: 0.913774  [51200/60000]\n",
      "loss: 0.861364  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.869334 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.904985  [    0/60000]\n",
      "loss: 0.963655  [ 6400/60000]\n",
      "loss: 0.735462  [12800/60000]\n",
      "loss: 0.915327  [19200/60000]\n",
      "loss: 0.811829  [25600/60000]\n",
      "loss: 0.816804  [32000/60000]\n",
      "loss: 0.893498  [38400/60000]\n",
      "loss: 0.838514  [44800/60000]\n",
      "loss: 0.860954  [51200/60000]\n",
      "loss: 0.815136  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.820423 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.838967  [    0/60000]\n",
      "loss: 0.914111  [ 6400/60000]\n",
      "loss: 0.678425  [12800/60000]\n",
      "loss: 0.871541  [19200/60000]\n",
      "loss: 0.775419  [25600/60000]\n",
      "loss: 0.769063  [32000/60000]\n",
      "loss: 0.854699  [38400/60000]\n",
      "loss: 0.808977  [44800/60000]\n",
      "loss: 0.822004  [51200/60000]\n",
      "loss: 0.779669  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.783360 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.787188  [    0/60000]\n",
      "loss: 0.873536  [ 6400/60000]\n",
      "loss: 0.634502  [12800/60000]\n",
      "loss: 0.838125  [19200/60000]\n",
      "loss: 0.747461  [25600/60000]\n",
      "loss: 0.732781  [32000/60000]\n",
      "loss: 0.822247  [38400/60000]\n",
      "loss: 0.786876  [44800/60000]\n",
      "loss: 0.791480  [51200/60000]\n",
      "loss: 0.750705  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.753485 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.744762  [    0/60000]\n",
      "loss: 0.838733  [ 6400/60000]\n",
      "loss: 0.599161  [12800/60000]\n",
      "loss: 0.811589  [19200/60000]\n",
      "loss: 0.724665  [25600/60000]\n",
      "loss: 0.704260  [32000/60000]\n",
      "loss: 0.793705  [38400/60000]\n",
      "loss: 0.768796  [44800/60000]\n",
      "loss: 0.766467  [51200/60000]\n",
      "loss: 0.725936  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.728250 \n",
      "\n",
      "New sparsity scheduled: 10 -- replacing with new mask\n",
      "New sparsity scheduled: 10 -- replacing with new mask\n",
      "Loaded mask from masks/masks_10.pt\n",
      "Loaded mask from masks/masks_10.pt\n",
      "Setting new learning rates\n",
      "Setting new learning rates\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.709023  [    0/60000]\n",
      "loss: 0.809519  [ 6400/60000]\n",
      "loss: 0.570968  [12800/60000]\n",
      "loss: 0.790277  [19200/60000]\n",
      "loss: 0.705209  [25600/60000]\n",
      "loss: 0.682853  [32000/60000]\n",
      "loss: 0.769773  [38400/60000]\n",
      "loss: 0.754809  [44800/60000]\n",
      "loss: 0.746563  [51200/60000]\n",
      "loss: 0.705363  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.708467 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.680613  [    0/60000]\n",
      "loss: 0.783362  [ 6400/60000]\n",
      "loss: 0.546800  [12800/60000]\n",
      "loss: 0.772280  [19200/60000]\n",
      "loss: 0.688897  [25600/60000]\n",
      "loss: 0.664478  [32000/60000]\n",
      "loss: 0.747295  [38400/60000]\n",
      "loss: 0.741561  [44800/60000]\n",
      "loss: 0.729201  [51200/60000]\n",
      "loss: 0.686666  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.689700 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.654385  [    0/60000]\n",
      "loss: 0.759615  [ 6400/60000]\n",
      "loss: 0.525885  [12800/60000]\n",
      "loss: 0.756643  [19200/60000]\n",
      "loss: 0.674304  [25600/60000]\n",
      "loss: 0.649276  [32000/60000]\n",
      "loss: 0.726309  [38400/60000]\n",
      "loss: 0.729710  [44800/60000]\n",
      "loss: 0.713969  [51200/60000]\n",
      "loss: 0.669944  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.672799 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.631141  [    0/60000]\n",
      "loss: 0.737994  [ 6400/60000]\n",
      "loss: 0.507626  [12800/60000]\n",
      "loss: 0.742650  [19200/60000]\n",
      "loss: 0.661290  [25600/60000]\n",
      "loss: 0.636140  [32000/60000]\n",
      "loss: 0.706787  [38400/60000]\n",
      "loss: 0.719125  [44800/60000]\n",
      "loss: 0.700636  [51200/60000]\n",
      "loss: 0.654835  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.657453 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.610526  [    0/60000]\n",
      "loss: 0.718409  [ 6400/60000]\n",
      "loss: 0.491414  [12800/60000]\n",
      "loss: 0.730026  [19200/60000]\n",
      "loss: 0.649796  [25600/60000]\n",
      "loss: 0.624764  [32000/60000]\n",
      "loss: 0.688697  [38400/60000]\n",
      "loss: 0.709674  [44800/60000]\n",
      "loss: 0.689162  [51200/60000]\n",
      "loss: 0.640851  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.643462 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.592032  [    0/60000]\n",
      "loss: 0.700506  [ 6400/60000]\n",
      "loss: 0.476845  [12800/60000]\n",
      "loss: 0.718432  [19200/60000]\n",
      "loss: 0.639464  [25600/60000]\n",
      "loss: 0.614775  [32000/60000]\n",
      "loss: 0.671891  [38400/60000]\n",
      "loss: 0.701353  [44800/60000]\n",
      "loss: 0.679342  [51200/60000]\n",
      "loss: 0.628007  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.630759 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.575423  [    0/60000]\n",
      "loss: 0.684207  [ 6400/60000]\n",
      "loss: 0.463733  [12800/60000]\n",
      "loss: 0.707707  [19200/60000]\n",
      "loss: 0.630083  [25600/60000]\n",
      "loss: 0.605966  [32000/60000]\n",
      "loss: 0.656544  [38400/60000]\n",
      "loss: 0.694174  [44800/60000]\n",
      "loss: 0.670900  [51200/60000]\n",
      "loss: 0.616049  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.619222 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.560471  [    0/60000]\n",
      "loss: 0.669354  [ 6400/60000]\n",
      "loss: 0.451915  [12800/60000]\n",
      "loss: 0.697705  [19200/60000]\n",
      "loss: 0.621474  [25600/60000]\n",
      "loss: 0.598119  [32000/60000]\n",
      "loss: 0.642463  [38400/60000]\n",
      "loss: 0.688109  [44800/60000]\n",
      "loss: 0.663757  [51200/60000]\n",
      "loss: 0.604874  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.608694 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.546839  [    0/60000]\n",
      "loss: 0.655736  [ 6400/60000]\n",
      "loss: 0.441213  [12800/60000]\n",
      "loss: 0.688330  [19200/60000]\n",
      "loss: 0.613508  [25600/60000]\n",
      "loss: 0.591032  [32000/60000]\n",
      "loss: 0.629518  [38400/60000]\n",
      "loss: 0.683033  [44800/60000]\n",
      "loss: 0.657662  [51200/60000]\n",
      "loss: 0.594323  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.599065 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.534356  [    0/60000]\n",
      "loss: 0.643256  [ 6400/60000]\n",
      "loss: 0.431413  [12800/60000]\n",
      "loss: 0.679476  [19200/60000]\n",
      "loss: 0.605923  [25600/60000]\n",
      "loss: 0.584553  [32000/60000]\n",
      "loss: 0.617680  [38400/60000]\n",
      "loss: 0.678935  [44800/60000]\n",
      "loss: 0.652395  [51200/60000]\n",
      "loss: 0.584260  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.590242 \n",
      "\n",
      "New sparsity is 0 -- removing mask\n",
      "New sparsity is 0 -- removing mask\n",
      "Restoring original learning rates\n",
      "Restoring original learning rates\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.522421  [    0/60000]\n",
      "loss: 0.631627  [ 6400/60000]\n",
      "loss: 0.422613  [12800/60000]\n",
      "loss: 0.670844  [19200/60000]\n",
      "loss: 0.598888  [25600/60000]\n",
      "loss: 0.578329  [32000/60000]\n",
      "loss: 0.607667  [38400/60000]\n",
      "loss: 0.676594  [44800/60000]\n",
      "loss: 0.649168  [51200/60000]\n",
      "loss: 0.575979  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.582990 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.512228  [    0/60000]\n",
      "loss: 0.621467  [ 6400/60000]\n",
      "loss: 0.414716  [12800/60000]\n",
      "loss: 0.663727  [19200/60000]\n",
      "loss: 0.592129  [25600/60000]\n",
      "loss: 0.573250  [32000/60000]\n",
      "loss: 0.598147  [38400/60000]\n",
      "loss: 0.674124  [44800/60000]\n",
      "loss: 0.645449  [51200/60000]\n",
      "loss: 0.567268  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.575927 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.502628  [    0/60000]\n",
      "loss: 0.612075  [ 6400/60000]\n",
      "loss: 0.407516  [12800/60000]\n",
      "loss: 0.656631  [19200/60000]\n",
      "loss: 0.585634  [25600/60000]\n",
      "loss: 0.568278  [32000/60000]\n",
      "loss: 0.589308  [38400/60000]\n",
      "loss: 0.672261  [44800/60000]\n",
      "loss: 0.642195  [51200/60000]\n",
      "loss: 0.558874  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.569399 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.493696  [    0/60000]\n",
      "loss: 0.603450  [ 6400/60000]\n",
      "loss: 0.400865  [12800/60000]\n",
      "loss: 0.649812  [19200/60000]\n",
      "loss: 0.579333  [25600/60000]\n",
      "loss: 0.563426  [32000/60000]\n",
      "loss: 0.581112  [38400/60000]\n",
      "loss: 0.670902  [44800/60000]\n",
      "loss: 0.639327  [51200/60000]\n",
      "loss: 0.550773  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.563346 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.485289  [    0/60000]\n",
      "loss: 0.595498  [ 6400/60000]\n",
      "loss: 0.394651  [12800/60000]\n",
      "loss: 0.643204  [19200/60000]\n",
      "loss: 0.573217  [25600/60000]\n",
      "loss: 0.558671  [32000/60000]\n",
      "loss: 0.573489  [38400/60000]\n",
      "loss: 0.670029  [44800/60000]\n",
      "loss: 0.636773  [51200/60000]\n",
      "loss: 0.542968  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.557720 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.477309  [    0/60000]\n",
      "loss: 0.588099  [ 6400/60000]\n",
      "loss: 0.388849  [12800/60000]\n",
      "loss: 0.636841  [19200/60000]\n",
      "loss: 0.567235  [25600/60000]\n",
      "loss: 0.553925  [32000/60000]\n",
      "loss: 0.566447  [38400/60000]\n",
      "loss: 0.669541  [44800/60000]\n",
      "loss: 0.634486  [51200/60000]\n",
      "loss: 0.535433  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.552488 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.469804  [    0/60000]\n",
      "loss: 0.581222  [ 6400/60000]\n",
      "loss: 0.383411  [12800/60000]\n",
      "loss: 0.630689  [19200/60000]\n",
      "loss: 0.561313  [25600/60000]\n",
      "loss: 0.549230  [32000/60000]\n",
      "loss: 0.559880  [38400/60000]\n",
      "loss: 0.669305  [44800/60000]\n",
      "loss: 0.632347  [51200/60000]\n",
      "loss: 0.528159  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.547619 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.462684  [    0/60000]\n",
      "loss: 0.574783  [ 6400/60000]\n",
      "loss: 0.378286  [12800/60000]\n",
      "loss: 0.624750  [19200/60000]\n",
      "loss: 0.555535  [25600/60000]\n",
      "loss: 0.544624  [32000/60000]\n",
      "loss: 0.553740  [38400/60000]\n",
      "loss: 0.669221  [44800/60000]\n",
      "loss: 0.630292  [51200/60000]\n",
      "loss: 0.521132  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.543070 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.455959  [    0/60000]\n",
      "loss: 0.568794  [ 6400/60000]\n",
      "loss: 0.373447  [12800/60000]\n",
      "loss: 0.619049  [19200/60000]\n",
      "loss: 0.549874  [25600/60000]\n",
      "loss: 0.540047  [32000/60000]\n",
      "loss: 0.547985  [38400/60000]\n",
      "loss: 0.669300  [44800/60000]\n",
      "loss: 0.628332  [51200/60000]\n",
      "loss: 0.514387  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.538816 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.449543  [    0/60000]\n",
      "loss: 0.563205  [ 6400/60000]\n",
      "loss: 0.368897  [12800/60000]\n",
      "loss: 0.613557  [19200/60000]\n",
      "loss: 0.544312  [25600/60000]\n",
      "loss: 0.535538  [32000/60000]\n",
      "loss: 0.542631  [38400/60000]\n",
      "loss: 0.669429  [44800/60000]\n",
      "loss: 0.626471  [51200/60000]\n",
      "loss: 0.507897  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.534827 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    sniper.step()\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"sniper_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now compare to the model trained without SNIPER. The SNIPER-trained model should converge faster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "init_values = torch.load(sniper_dir / 'init_values.pt', map_location=device)\n",
    "model.load_state_dict(init_values)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=optim_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304559  [    0/60000]\n",
      "loss: 2.291708  [ 6400/60000]\n",
      "loss: 2.276042  [12800/60000]\n",
      "loss: 2.267381  [19200/60000]\n",
      "loss: 2.247044  [25600/60000]\n",
      "loss: 2.228423  [32000/60000]\n",
      "loss: 2.235480  [38400/60000]\n",
      "loss: 2.204573  [44800/60000]\n",
      "loss: 2.200688  [51200/60000]\n",
      "loss: 2.166080  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 2.164025 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.177513  [    0/60000]\n",
      "loss: 2.165572  [ 6400/60000]\n",
      "loss: 2.116897  [12800/60000]\n",
      "loss: 2.125358  [19200/60000]\n",
      "loss: 2.068360  [25600/60000]\n",
      "loss: 2.028746  [32000/60000]\n",
      "loss: 2.048096  [38400/60000]\n",
      "loss: 1.975801  [44800/60000]\n",
      "loss: 1.973027  [51200/60000]\n",
      "loss: 1.901443  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.906071 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.940768  [    0/60000]\n",
      "loss: 1.910886  [ 6400/60000]\n",
      "loss: 1.804289  [12800/60000]\n",
      "loss: 1.829366  [19200/60000]\n",
      "loss: 1.716968  [25600/60000]\n",
      "loss: 1.684815  [32000/60000]\n",
      "loss: 1.691515  [38400/60000]\n",
      "loss: 1.596511  [44800/60000]\n",
      "loss: 1.610441  [51200/60000]\n",
      "loss: 1.505004  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.533477 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.600291  [    0/60000]\n",
      "loss: 1.567644  [ 6400/60000]\n",
      "loss: 1.423219  [12800/60000]\n",
      "loss: 1.482666  [19200/60000]\n",
      "loss: 1.360680  [25600/60000]\n",
      "loss: 1.367577  [32000/60000]\n",
      "loss: 1.370885  [38400/60000]\n",
      "loss: 1.294867  [44800/60000]\n",
      "loss: 1.329473  [51200/60000]\n",
      "loss: 1.226640  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.262711 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.342394  [    0/60000]\n",
      "loss: 1.325248  [ 6400/60000]\n",
      "loss: 1.162412  [12800/60000]\n",
      "loss: 1.259598  [19200/60000]\n",
      "loss: 1.130609  [25600/60000]\n",
      "loss: 1.166064  [32000/60000]\n",
      "loss: 1.179051  [38400/60000]\n",
      "loss: 1.112416  [44800/60000]\n",
      "loss: 1.157005  [51200/60000]\n",
      "loss: 1.069166  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.097385 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.172535  [    0/60000]\n",
      "loss: 1.174647  [ 6400/60000]\n",
      "loss: 0.992932  [12800/60000]\n",
      "loss: 1.123158  [19200/60000]\n",
      "loss: 0.990995  [25600/60000]\n",
      "loss: 1.032584  [32000/60000]\n",
      "loss: 1.062676  [38400/60000]\n",
      "loss: 0.997931  [44800/60000]\n",
      "loss: 1.045073  [51200/60000]\n",
      "loss: 0.971826  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.991071 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.053848  [    0/60000]\n",
      "loss: 1.077068  [ 6400/60000]\n",
      "loss: 0.877390  [12800/60000]\n",
      "loss: 1.032948  [19200/60000]\n",
      "loss: 0.903698  [25600/60000]\n",
      "loss: 0.938062  [32000/60000]\n",
      "loss: 0.987013  [38400/60000]\n",
      "loss: 0.923339  [44800/60000]\n",
      "loss: 0.966491  [51200/60000]\n",
      "loss: 0.905966  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.917776 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.964907  [    0/60000]\n",
      "loss: 1.008532  [ 6400/60000]\n",
      "loss: 0.794428  [12800/60000]\n",
      "loss: 0.968489  [19200/60000]\n",
      "loss: 0.845152  [25600/60000]\n",
      "loss: 0.867686  [32000/60000]\n",
      "loss: 0.933185  [38400/60000]\n",
      "loss: 0.873037  [44800/60000]\n",
      "loss: 0.908365  [51200/60000]\n",
      "loss: 0.857576  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.864149 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.895814  [    0/60000]\n",
      "loss: 0.956412  [ 6400/60000]\n",
      "loss: 0.731878  [12800/60000]\n",
      "loss: 0.919792  [19200/60000]\n",
      "loss: 0.803161  [25600/60000]\n",
      "loss: 0.813571  [32000/60000]\n",
      "loss: 0.891988  [38400/60000]\n",
      "loss: 0.837630  [44800/60000]\n",
      "loss: 0.863823  [51200/60000]\n",
      "loss: 0.819837  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.822940 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.840181  [    0/60000]\n",
      "loss: 0.913956  [ 6400/60000]\n",
      "loss: 0.683104  [12800/60000]\n",
      "loss: 0.881556  [19200/60000]\n",
      "loss: 0.771151  [25600/60000]\n",
      "loss: 0.771430  [32000/60000]\n",
      "loss: 0.858292  [38400/60000]\n",
      "loss: 0.811449  [44800/60000]\n",
      "loss: 0.828704  [51200/60000]\n",
      "loss: 0.789119  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.789922 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.794320  [    0/60000]\n",
      "loss: 0.877739  [ 6400/60000]\n",
      "loss: 0.643655  [12800/60000]\n",
      "loss: 0.850771  [19200/60000]\n",
      "loss: 0.745482  [25600/60000]\n",
      "loss: 0.737982  [32000/60000]\n",
      "loss: 0.829239  [38400/60000]\n",
      "loss: 0.791010  [44800/60000]\n",
      "loss: 0.800448  [51200/60000]\n",
      "loss: 0.763149  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.762454 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.755445  [    0/60000]\n",
      "loss: 0.845911  [ 6400/60000]\n",
      "loss: 0.610914  [12800/60000]\n",
      "loss: 0.825558  [19200/60000]\n",
      "loss: 0.723994  [25600/60000]\n",
      "loss: 0.710914  [32000/60000]\n",
      "loss: 0.803131  [38400/60000]\n",
      "loss: 0.774023  [44800/60000]\n",
      "loss: 0.776992  [51200/60000]\n",
      "loss: 0.740591  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.738887 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.721781  [    0/60000]\n",
      "loss: 0.817270  [ 6400/60000]\n",
      "loss: 0.583095  [12800/60000]\n",
      "loss: 0.804203  [19200/60000]\n",
      "loss: 0.705462  [25600/60000]\n",
      "loss: 0.688613  [32000/60000]\n",
      "loss: 0.779231  [38400/60000]\n",
      "loss: 0.759417  [44800/60000]\n",
      "loss: 0.756863  [51200/60000]\n",
      "loss: 0.720504  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.718142 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.692203  [    0/60000]\n",
      "loss: 0.791168  [ 6400/60000]\n",
      "loss: 0.559044  [12800/60000]\n",
      "loss: 0.785793  [19200/60000]\n",
      "loss: 0.689036  [25600/60000]\n",
      "loss: 0.670030  [32000/60000]\n",
      "loss: 0.757108  [38400/60000]\n",
      "loss: 0.746394  [44800/60000]\n",
      "loss: 0.739310  [51200/60000]\n",
      "loss: 0.702361  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.699533 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.665974  [    0/60000]\n",
      "loss: 0.767320  [ 6400/60000]\n",
      "loss: 0.537841  [12800/60000]\n",
      "loss: 0.769489  [19200/60000]\n",
      "loss: 0.674417  [25600/60000]\n",
      "loss: 0.654308  [32000/60000]\n",
      "loss: 0.736309  [38400/60000]\n",
      "loss: 0.734713  [44800/60000]\n",
      "loss: 0.723854  [51200/60000]\n",
      "loss: 0.685753  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.682689 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.642561  [    0/60000]\n",
      "loss: 0.745584  [ 6400/60000]\n",
      "loss: 0.519103  [12800/60000]\n",
      "loss: 0.754970  [19200/60000]\n",
      "loss: 0.661304  [25600/60000]\n",
      "loss: 0.640743  [32000/60000]\n",
      "loss: 0.716999  [38400/60000]\n",
      "loss: 0.724117  [44800/60000]\n",
      "loss: 0.710329  [51200/60000]\n",
      "loss: 0.670531  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.667326 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.621585  [    0/60000]\n",
      "loss: 0.725657  [ 6400/60000]\n",
      "loss: 0.502346  [12800/60000]\n",
      "loss: 0.741813  [19200/60000]\n",
      "loss: 0.649432  [25600/60000]\n",
      "loss: 0.628920  [32000/60000]\n",
      "loss: 0.698935  [38400/60000]\n",
      "loss: 0.714584  [44800/60000]\n",
      "loss: 0.698452  [51200/60000]\n",
      "loss: 0.656568  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.653266 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.602692  [    0/60000]\n",
      "loss: 0.707379  [ 6400/60000]\n",
      "loss: 0.487260  [12800/60000]\n",
      "loss: 0.729646  [19200/60000]\n",
      "loss: 0.638797  [25600/60000]\n",
      "loss: 0.618562  [32000/60000]\n",
      "loss: 0.682129  [38400/60000]\n",
      "loss: 0.706072  [44800/60000]\n",
      "loss: 0.687997  [51200/60000]\n",
      "loss: 0.643689  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.640386 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.585608  [    0/60000]\n",
      "loss: 0.690638  [ 6400/60000]\n",
      "loss: 0.473628  [12800/60000]\n",
      "loss: 0.718470  [19200/60000]\n",
      "loss: 0.629255  [25600/60000]\n",
      "loss: 0.609436  [32000/60000]\n",
      "loss: 0.666542  [38400/60000]\n",
      "loss: 0.698544  [44800/60000]\n",
      "loss: 0.678847  [51200/60000]\n",
      "loss: 0.631765  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.628581 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.570195  [    0/60000]\n",
      "loss: 0.675299  [ 6400/60000]\n",
      "loss: 0.461308  [12800/60000]\n",
      "loss: 0.708197  [19200/60000]\n",
      "loss: 0.620586  [25600/60000]\n",
      "loss: 0.601310  [32000/60000]\n",
      "loss: 0.652033  [38400/60000]\n",
      "loss: 0.692037  [44800/60000]\n",
      "loss: 0.670898  [51200/60000]\n",
      "loss: 0.620704  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.617743 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.556092  [    0/60000]\n",
      "loss: 0.661240  [ 6400/60000]\n",
      "loss: 0.450068  [12800/60000]\n",
      "loss: 0.698630  [19200/60000]\n",
      "loss: 0.612686  [25600/60000]\n",
      "loss: 0.594120  [32000/60000]\n",
      "loss: 0.638664  [38400/60000]\n",
      "loss: 0.686443  [44800/60000]\n",
      "loss: 0.664017  [51200/60000]\n",
      "loss: 0.610328  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.607788 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.543261  [    0/60000]\n",
      "loss: 0.648299  [ 6400/60000]\n",
      "loss: 0.439793  [12800/60000]\n",
      "loss: 0.689648  [19200/60000]\n",
      "loss: 0.605319  [25600/60000]\n",
      "loss: 0.587543  [32000/60000]\n",
      "loss: 0.626272  [38400/60000]\n",
      "loss: 0.681688  [44800/60000]\n",
      "loss: 0.658103  [51200/60000]\n",
      "loss: 0.600560  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.598649 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.531542  [    0/60000]\n",
      "loss: 0.636403  [ 6400/60000]\n",
      "loss: 0.430442  [12800/60000]\n",
      "loss: 0.681101  [19200/60000]\n",
      "loss: 0.598326  [25600/60000]\n",
      "loss: 0.581520  [32000/60000]\n",
      "loss: 0.614809  [38400/60000]\n",
      "loss: 0.677734  [44800/60000]\n",
      "loss: 0.652996  [51200/60000]\n",
      "loss: 0.591288  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.590236 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.520773  [    0/60000]\n",
      "loss: 0.625423  [ 6400/60000]\n",
      "loss: 0.421816  [12800/60000]\n",
      "loss: 0.672989  [19200/60000]\n",
      "loss: 0.591608  [25600/60000]\n",
      "loss: 0.575895  [32000/60000]\n",
      "loss: 0.604205  [38400/60000]\n",
      "loss: 0.674445  [44800/60000]\n",
      "loss: 0.648558  [51200/60000]\n",
      "loss: 0.582428  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.582485 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.510766  [    0/60000]\n",
      "loss: 0.615335  [ 6400/60000]\n",
      "loss: 0.413851  [12800/60000]\n",
      "loss: 0.665239  [19200/60000]\n",
      "loss: 0.585169  [25600/60000]\n",
      "loss: 0.570660  [32000/60000]\n",
      "loss: 0.594397  [38400/60000]\n",
      "loss: 0.671862  [44800/60000]\n",
      "loss: 0.644672  [51200/60000]\n",
      "loss: 0.573936  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.575330 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.501436  [    0/60000]\n",
      "loss: 0.605989  [ 6400/60000]\n",
      "loss: 0.406466  [12800/60000]\n",
      "loss: 0.657829  [19200/60000]\n",
      "loss: 0.578936  [25600/60000]\n",
      "loss: 0.565641  [32000/60000]\n",
      "loss: 0.585289  [38400/60000]\n",
      "loss: 0.669880  [44800/60000]\n",
      "loss: 0.641302  [51200/60000]\n",
      "loss: 0.565682  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.568712 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.492678  [    0/60000]\n",
      "loss: 0.597355  [ 6400/60000]\n",
      "loss: 0.399598  [12800/60000]\n",
      "loss: 0.650737  [19200/60000]\n",
      "loss: 0.572826  [25600/60000]\n",
      "loss: 0.560774  [32000/60000]\n",
      "loss: 0.576797  [38400/60000]\n",
      "loss: 0.668322  [44800/60000]\n",
      "loss: 0.638316  [51200/60000]\n",
      "loss: 0.557704  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.562577 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.484464  [    0/60000]\n",
      "loss: 0.589357  [ 6400/60000]\n",
      "loss: 0.393172  [12800/60000]\n",
      "loss: 0.644005  [19200/60000]\n",
      "loss: 0.566837  [25600/60000]\n",
      "loss: 0.556006  [32000/60000]\n",
      "loss: 0.568946  [38400/60000]\n",
      "loss: 0.667188  [44800/60000]\n",
      "loss: 0.635667  [51200/60000]\n",
      "loss: 0.549969  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.556878 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.476682  [    0/60000]\n",
      "loss: 0.581941  [ 6400/60000]\n",
      "loss: 0.387134  [12800/60000]\n",
      "loss: 0.637466  [19200/60000]\n",
      "loss: 0.560952  [25600/60000]\n",
      "loss: 0.551288  [32000/60000]\n",
      "loss: 0.561667  [38400/60000]\n",
      "loss: 0.666434  [44800/60000]\n",
      "loss: 0.633221  [51200/60000]\n",
      "loss: 0.542398  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.551570 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.469310  [    0/60000]\n",
      "loss: 0.575096  [ 6400/60000]\n",
      "loss: 0.381406  [12800/60000]\n",
      "loss: 0.631196  [19200/60000]\n",
      "loss: 0.555204  [25600/60000]\n",
      "loss: 0.546655  [32000/60000]\n",
      "loss: 0.554890  [38400/60000]\n",
      "loss: 0.666039  [44800/60000]\n",
      "loss: 0.631061  [51200/60000]\n",
      "loss: 0.535010  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.546612 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"original_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}